modelSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/modelhpc'
logSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/loghpc'
urdfPath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/panda_gym/envs/robots/panda.urdf'
bufferPath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/bufferhpc/'
#modelSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/modelhpc'
#logSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/loghpc'
#urdfPath: '/kuacc/users/tbal21/panda_gym/envs/robots/panda.urdf'
#bufferPath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/bufferhpc'
expNumber: 5
envName: "PandaReach-v2"   
total_timesteps: 1000
learning_starts: 900
mode: False  
algorithm: "TQC" 
policy: "MultiInputPolicy"
render: True
gamma: 0.95
batch_size: 32
learning_rate: 0.001
n_envs: 1
testSamples: 3
max_episode_steps: 800
verbose: 1 
pseudoI: False 
save_freq: 50000
sampleJointAnglesGoal: False # If this is false, then you should determine goal_range value
goal_range: 0.4
randomStart: False # If this is false, then you should determine neutral joint angles.. These values are defined in myrobot.py
lambdaErr: 20.0
accelerationConstant: 0.005
velocityConstant: 0.0
velocityThreshold: 0.08
thresholdConstant: 0.0
alpha: 10
# TODO env = VecNormalize(env, norm_obs=True, norm_reward=True,
                   #clip_obs=10.)

                                     
ent_coef: 'auto'        

normalize: True       
#batch_size: 2048
buffer_size: 300000
replay_buffer_kwargs : {'online_sampling' : True,
             'goal_selection_strategy' : 'future',
             'n_sampled_goal' : 4}
policy_kwargs : {'net_arch' : [128, 128], 
                 'n_critics' : 1}



#c = dict(activation_fn=th.nn.Tanh, net_arch=[dict(pi=[128, 128,128], vf=[128,128, 128])])
#policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,net_arch=dict(pi=[128, 128], qf=[128, 128]))




