modelSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/modelhpc'
logSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/loghpc'
urdfPath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/panda_gym/envs/robots/panda.urdf'
#modelSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/modelhpc'
#logSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/loghpc'
#urdfPath: '/kuacc/users/tbal21/panda_gym/envs/robots/panda.urdf'
expNumber: 11
curriNumber: 13
envName: "PandaReach-v2"             
total_timesteps: !!float 1000
mode: False                           
algorithm: "PPO"                    
policy: "MultiInputPolicy"           
render: True                        
gamma: 0.95 
n_steps: 256
batch_size: 256
learning_rate: 0.0003
n_envs: 1
testSamples: 4
max_episode_steps: 800
verbose: 1
pseudoI: False
save_freq: !!float 100e6 
sampleJointAnglesGoal: False # If this is false, then you should determine goal_range value
goal_range: 0.4
randomStart: False # If this is false, then you should determine neutral joint angles.. These values are defined in myrobot.py
curriLearning: False  # If this is true, than determine curriNumber
lambdaErr: 20.0
accelerationConstant: 0.005
velocityConstant: 0.0
velocityThreshold: 0.08
thresholdConstant: 0.0

# TODO env = VecNormalize(env, norm_obs=True, norm_reward=True,
                   #clip_obs=10.)

######
#policy_kwargs= dict(
#                    log_std_init=-2,
#                 ortho_init=False,
#                    activation_fn=th.nn.ReLU,
#                    net_arch=dict(pi=[128, 128], vf=[128, 128])
#                  )
