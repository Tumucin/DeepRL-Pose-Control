#################### PATHS - LOCAL- ####################
#modelSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/modelhpc'
#logSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/loghpc'
#failedSamplesSavePath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/failedSamples'
#urdfPath: '/home/tumu/anaconda3/envs/stableBaselines/panda-gym/panda_gym/envs/robots/panda.urdf'
#file_name: "franka_panda/panda.urdf"
#datasetPath: "/home/tumu/anaconda3/envs/stableBaselines/panda-gym/panda_gym/envs/dataset/pandaversion1"

#################### PATHS - HPC- ####################
modelSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/modelhpc'
logSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/loghpc'
failedSamplesSavePath: '/scratch/users/tbal21/.conda/envs/stableBaselines/panda-gym/failedSamples'
urdfPath: '/kuacc/users/tbal21/.conda/envs/stableBaselines/panda-gym/panda_gym/envs/robots/panda.urdf'
file_name: "franka_panda/panda.urdf"
datasetPath: '/kuacc/users/tbal21/.conda/envs/stableBaselines/panda-gym/panda_gym/envs/dataset/pandaversion1'

expNumber: 229                        # Experiement Number  
envName: "PandaReach-v2"              # Environment Name    
ee_link: 11                           # End effector link ID
body_name: 'panda'                    # Name of the robot   
baseLinkName: 'panda_link0'           # The name of the base link of a robot
eeLinkName: 'panda_grasptarget'       # The name of the ee link of a robot
base_position: [-0.6, 0.0, 0.0]       # Base position of the base link
total_timesteps: !!float 32768000     # Total timesteps for training
mode: True                           # True: Training, False: Testing
algorithm: "PPO"                      # Name of the algorithm
policy: "MultiInputPolicy"            # Policy network
render: False                          # True: Render the sim, False: No Rendering
gamma: 0.95                           # Discount Factor
n_steps: 4096                         # The number of steps to run for each environment per update
n_epochs: 10                          # Number of epoch when optimizing the surrogate loss
batch_size: 2048                      # Mini-batch size
learning_rate: 0.0003                 # The learning rate
n_envs: 16                            # Number of environments
testSamples: 1000                     # Number of samples for evaluation
testSampleOnTraining: 500             # Number of samples for evaluation while training (Curriculum Learning)
evalFreqOnTraining: 2000000           # Number of timesteps per evaluation (Curriculum Learning)
max_episode_steps: 800                # Episode length
verbose: 1                            # Visualize the log of training
pseudoI: False                         # Use PseudoInverse to calculate q_dot (Note that there are Position Only and FullJacobian methods)
networkOutput: True                  # Use Network output
save_freq: !!float 100e6              # Number of timesteps for saving checkpoints
sampleJointAnglesGoal: True           # Sample random goals(Position+Orientation). If this is FALSE, then you should determine goal_range value
goal_range: 0.5                       # Goal range for sampling the goals(desired points)
randomStart: True                     # Sample random starting goals(Position+Orientation) If this is FALSE, then you should determine neutral_joint_values below
neutral_joint_values: [0.00, 0.41, 0.00, -1.85, 0.00, 2.26, 0.79, 0.00, 0.00]
lambdaErr: 100.0                      # One of the parameters for reward function exp(-k*d*d)
orientationConstant: 1                # One of the parameters for reward function exp(-k*d*d)
addOrientation: False                 # Use orientation information as states. If FALSE, do not use
accelerationConstant: 0.0             # One of the parameters for reward function k*norm(jntAcceleration)
velocityConstant: 0.1                 # One of the parameters for reward function k*norm(jntVelocity)
velocityNormThreshold: 0.05
thresholdConstant: 0.0
alpha: 1.0                            # One of the parameters for reward function k*norm(jntVelocity)/(1+alpha*d)
activation_fn: 0                      # Activation Function: Tanh: 0, RELU:1
hiddenUnits: 128                      # The number of neurons in the hidden layers
CurriLearning: True                  # Use Curriculum Learning or not. If this is FALSE, then you should edit the variables below
curriculumFirstWorkspaceId: "W1"
finalWorkspaceID: "W4"
workspaceLen: 4
rmseThreshold: 0.012                  # RMSE threshold to go to the next Workspace (Curriculum Learning)
maeThreshold: 0.012                   # MAE threshold to go to the next Workspace (Curriculum Learning)
avgJntVelThreshold: 0.1               # AverageJntVel threshold to go to the next Workspace (Curriculum Learning)
visualizeFailedSamples: False
net_arch: [{'pi': [128, 128, 128],
            'vf': [128, 128, 128]}]        # Network Architecture
collisionConstant: 100